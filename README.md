# Bandits
Collection of code and explanatory notebooks building through the steps of simple Multi-armed bandit with various sampling strategies, to contextual bandits and in to simple model-based RL. 
