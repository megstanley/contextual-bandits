# Bandits
Ongoing collection of code and explanatory notebooks building through the steps of simple Multi-armed bandit with various sampling strategies, to contextual bandits and in to simple model-based RL. Used for tutorial at Faculty. 

TODO: clean up 
