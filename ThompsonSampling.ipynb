{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling (not in the Bernoulli case)\n",
    "\n",
    "Here, I discuss the form of an agent (and Bandit) suitable when there is a lognormal posterior for the expected value of the distribution of the payback when a lever is pulled. This is following chapter 4 of [this book](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf), and uses this specific example to demonstrate how this works. \n",
    "\n",
    "(May restructure classes again for better generalisation).\n",
    "\n",
    "## **The Problem**\n",
    "\n",
    "Imagine we want to solve an online routing problem. The aim is to get from one side to the other of this graph while incurring the minimum time penalty. The challenge is that we do not know precisely how long traversing any of the edges will take because they are drawn from *as yet unknown* distributions with underlying mean $\\theta_{edge}$. \n",
    "\n",
    "Were the edge's distributions known, then you would just find the path that minimised $\\theta_{t=1, e} + \\theta_{t=2, e} + \\theta_{t=3, e} + ... $ \n",
    "\n",
    "However, you just don't have access to that information. All you can do is try a path and intelligently explore while trying to keep the cost incurred to a minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![title](img/picture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general parent class for an agent that is playing a bandit problem.\n",
    "\n",
    "class StandardAgent:\n",
    "    \n",
    "    def __init__(self, bandit):\n",
    "        #only require the bandit to initialise\n",
    "        \n",
    "        #general attributes of all agents\n",
    "        self.bandit = bandit\n",
    "        self.pull_record = [0] * self.bandit.k\n",
    "        self.action_sequence = []\n",
    "        \n",
    "        #for keeping track of how the agent learns\n",
    "        self.num_optimal_pulls = 0\n",
    "        self.optimal_trajectory = []\n",
    "        \n",
    "        #estimated Q is the value of each action, or the expected reward of each action\n",
    "        self.estimatedQ = [0] * self.bandit.k\n",
    "        \n",
    "        self.Q_trajectory = [[] for i in range(self.bandit.k)]\n",
    "        \n",
    "        #keep track of the agent's knowledge of the probability distributions\n",
    "        #cumulative regret\n",
    "        self.total_regret = 0\n",
    "        #keeping an eye on how regret changes\n",
    "        self.regrets_trajectory = []\n",
    "        \n",
    "    def update_R(self, lever):\n",
    "        \n",
    "        self.total_regret += (self.bandit.best_reward - self.bandit.expected_reward[lever])\n",
    "        self.regrets_trajectory.append(self.total_regret)\n",
    "        \n",
    "    def choose_lever(self):\n",
    "        #in child classes this involves getting back the lever and returning the reward\n",
    "        raise NotImplementedError('Not a method in this parent class.')\n",
    "        \n",
    "    @property\n",
    "    def Q_values(self):\n",
    "        raise NotImplementedError('Not a method in this parent class.')\n",
    "        \n",
    "    def run_trial(self, n_steps):\n",
    "        for step in range(n_steps):\n",
    "            \n",
    "            lever = self.choose_lever()\n",
    "            \n",
    "            # Update the array keeping track of how many times each lever has been pulled\n",
    "            self.pull_record[lever] += 1\n",
    "            \n",
    "            # update whether lever was best\n",
    "            if lever == self.bandit.best_arm:\n",
    "                self.num_optimal_pulls += 1\n",
    "            self.optimal_trajectory.append(self.num_optimal_pulls / np.sum(self.pull_record))\n",
    "            \n",
    "            self.action_sequence.append(lever)\n",
    "            # update the regret \n",
    "            self.update_R(lever)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#child class that can perform Thompson sampling, with lognormal dist prior.\n",
    "\n",
    "class logNormalThompsonAgent(StandardAgent):\n",
    "    def __init__(self, bandit, mu = 0, sigma = 100):\n",
    "        super(logNormalThompsonAgent, self).__init__(bandit)\n",
    "        \n",
    "        #sample from the prior distribution over the parameters for each arm (default uninformative prior)\n",
    "        self.theta_estimate = [np.random.lognormal(mu, sigma) for k in self.bandit.k]\n",
    "        \n",
    "    def choose_lever(self):\n",
    "        \n",
    "        #using beta prior with initial uniform distribution in first pass\n",
    "        for i in range(0, self.bandit.k):\n",
    "            self.theta_estimate[i] = np.random.beta(self.a[i], self.b[i])\n",
    "        \n",
    "        #choose the largest resulting theta estimate\n",
    "        lever = np.argmax(self.theta_estimate)\n",
    "        reward = self.bandit.pull_lever(lever)\n",
    "        \n",
    "        #update the posterior distribution over theta (update parameters of that dist)\n",
    "        self.a[lever] += reward\n",
    "        self.b[lever] += 1-reward\n",
    "        \n",
    "        self.update_Q()\n",
    "        \n",
    "        return lever\n",
    "    \n",
    "    def update_Q(self):\n",
    "        #update the Q values\n",
    "        for k in range(0, self.bandit.k):\n",
    "            self.estimatedQ[k] = self.a[k]/(self.a[k] + self.b[k]) #estimate of the Q values for each lever, here they are expectation values\n",
    "            #print(self.Q_trajectory[k])\n",
    "            self.Q_trajectory[k].append(self.estimatedQ[k])\n",
    "    \n",
    "        \n",
    "    #record the reward as an update to the 'Q value' for this action:\n",
    "    @property\n",
    "    def Q_values(self):\n",
    "        return self.estimatedQ\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
