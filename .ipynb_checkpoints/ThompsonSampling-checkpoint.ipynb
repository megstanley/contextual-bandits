{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling\n",
    "\n",
    "Here, I discuss the form of an agent (and Bandit) suitable when there is a lognormal posterior for the expected value of the distribution of the payback when a lever is pulled. This is following chapter 4 of [this book](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf).\n",
    "\n",
    "(May restructure classes again for better generalisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general parent class for an agent that is playing a bandit problem.\n",
    "\n",
    "class StandardAgent:\n",
    "    \n",
    "    def __init__(self, bandit):\n",
    "        #only require the bandit to initialise\n",
    "        \n",
    "        #general attributes of all agents\n",
    "        self.bandit = bandit\n",
    "        self.pull_record = [0] * self.bandit.k\n",
    "        self.action_sequence = []\n",
    "        \n",
    "        #for keeping track of how the agent learns\n",
    "        self.num_optimal_pulls = 0\n",
    "        self.optimal_trajectory = []\n",
    "        \n",
    "        #estimated Q is the value of each action, or the expected reward of each action\n",
    "        self.estimatedQ = [0] * self.bandit.k\n",
    "        \n",
    "        self.Q_trajectory = [[] for i in range(self.bandit.k)]\n",
    "        \n",
    "        #keep track of the agent's knowledge of the probability distributions\n",
    "        #cumulative regret\n",
    "        self.total_regret = 0\n",
    "        #keeping an eye on how regret changes\n",
    "        self.regrets_trajectory = []\n",
    "        \n",
    "    def update_R(self, lever):\n",
    "        \n",
    "        self.total_regret += (self.bandit.best_reward - self.bandit.expected_reward[lever])\n",
    "        self.regrets_trajectory.append(self.total_regret)\n",
    "        \n",
    "    def choose_lever(self):\n",
    "        #in child classes this involves getting back the lever and returning the reward\n",
    "        raise NotImplementedError('Not a method in this parent class.')\n",
    "        \n",
    "    @property\n",
    "    def Q_values(self):\n",
    "        raise NotImplementedError('Not a method in this parent class.')\n",
    "        \n",
    "    def run_trial(self, n_steps):\n",
    "        for step in range(n_steps):\n",
    "            \n",
    "            lever = self.choose_lever()\n",
    "            \n",
    "            # Update the array keeping track of how many times each lever has been pulled\n",
    "            self.pull_record[lever] += 1\n",
    "            \n",
    "            # update whether lever was best\n",
    "            if lever == self.bandit.best_arm:\n",
    "                self.num_optimal_pulls += 1\n",
    "            self.optimal_trajectory.append(self.num_optimal_pulls / np.sum(self.pull_record))\n",
    "            \n",
    "            self.action_sequence.append(lever)\n",
    "            # update the regret \n",
    "            self.update_R(lever)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#child class that can perform Thompson sampling, with lognormal dist prior.\n",
    "\n",
    "class logNormalThompsonAgent(StandardAgent):\n",
    "    def __init__(self, bandit, mu = 0, sigma = 100):\n",
    "        super(logNormalThompsonAgent, self).__init__(bandit)\n",
    "        \n",
    "        #sample from the prior distribution over the parameters for each arm (default uninformative prior)\n",
    "        self.theta_estimate = [np.random.lognormal(mu, sigma) for k in self.bandit.k]\n",
    "        \n",
    "    def choose_lever(self):\n",
    "        \n",
    "        #using beta prior with initial uniform distribution in first pass\n",
    "        for i in range(0, self.bandit.k):\n",
    "            self.theta_estimate[i] = np.random.beta(self.a[i], self.b[i])\n",
    "        \n",
    "        #choose the largest resulting theta estimate\n",
    "        lever = np.argmax(self.theta_estimate)\n",
    "        reward = self.bandit.pull_lever(lever)\n",
    "        \n",
    "        #update the posterior distribution over theta (update parameters of that dist)\n",
    "        self.a[lever] += reward\n",
    "        self.b[lever] += 1-reward\n",
    "        \n",
    "        self.update_Q()\n",
    "        \n",
    "        return lever\n",
    "    \n",
    "    def update_Q(self):\n",
    "        #update the Q values\n",
    "        for k in range(0, self.bandit.k):\n",
    "            self.estimatedQ[k] = self.a[k]/(self.a[k] + self.b[k]) #estimate of the Q values for each lever, here they are expectation values\n",
    "            #print(self.Q_trajectory[k])\n",
    "            self.Q_trajectory[k].append(self.estimatedQ[k])\n",
    "    \n",
    "        \n",
    "    #record the reward as an update to the 'Q value' for this action:\n",
    "    @property\n",
    "    def Q_values(self):\n",
    "        return self.estimatedQ\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
